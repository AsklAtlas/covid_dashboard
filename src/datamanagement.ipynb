{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "# import schedule\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "ETL\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.data.gouv.fr/fr/datasets/indicateurs-de-suivi-de-lepidemie-de-covid-19/\n",
    "https://www.data.gouv.fr/fr/datasets/donnees-relatives-aux-resultats-des-tests-virologiques-covid-19/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-38-bd4bda15555c>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-38-bd4bda15555c>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    def extract(local=True):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import schedule\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def job():\n",
    "    \n",
    "    def extract(local=True):\n",
    "\n",
    "        if local:\n",
    "            df = pd.read_csv(\"../data/chiffres-cles.csv\")\n",
    "            quatre_taux = pd.read_csv(\"../data/table-indicateurs-open-data-dep-serie.csv\")\n",
    "        else: \n",
    "            url = \"https://github.com/opencovid19-fr/data/raw/master/dist/chiffres-cles.csv\"\n",
    "            s = requests.get(url).content\n",
    "            df = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "            url= \"https://www.data.gouv.fr/fr/datasets/r/4acad602-d8b1-4516-bc71-7d5574d5f33e\"\n",
    "            s=requests.get(url).content\n",
    "            quatre_taux = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "        dpt = pd.read_csv(\"../data/departements-region.csv\")\n",
    "        reg = pd.read_csv(\"../data/regions-france.csv\")\n",
    "        pop = pd.read_csv(\"../data/estim_pop.csv\")\n",
    "\n",
    "\n",
    "        df[\"date\"] = df[\"date\"].replace(\"_\", \"-\", regex=True)\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "        df[\"dep_code\"] = df[(df['maille_code'].str.startswith('DEP')) | (df['maille_code'].str.startswith('COM'))][\"maille_code\"]\\\n",
    "        .replace(\".*-\", \"\", regex=True)\n",
    "        df[\"reg_code\"] = df[df['maille_code'].str.startswith('REG')][\"maille_code\"]\\\n",
    "        .replace(\".*-\", \"\", regex=True).astype(\"int64\")\n",
    "\n",
    "        quatre_taux[\"extract_date\"] = pd.to_datetime(quatre_taux[\"extract_date\"])\n",
    "\n",
    "\n",
    "        geo = dpt.merge(reg, right_on= 'nom_region', left_on='region_name')\\\n",
    "        .rename(columns = {'num_dep':'code_dep'})\\\n",
    "        .drop(columns=[\"nom_region\"])[['code_region', 'region_name', 'code_dep', 'dep_name']]\n",
    "\n",
    "        pop[[\"2020\", \"2021\"]] = pop[[\"2020\", \"2021\"]].replace(\"\\s\", \"\", regex=True).astype(int)\n",
    "\n",
    "        return [df, quatre_taux, geo, pop]\n",
    "\n",
    "\n",
    "    def transform(dataframe):\n",
    "\n",
    "        df, quatre_taux, geo, pop = dataframe[0], dataframe[1], dataframe[2], dataframe[3]\n",
    "\n",
    "        df_clean = df.merge(geo, right_on=\"code_dep\", left_on='dep_code')[['date', 'code_region', 'region_name', 'code_dep',\n",
    "               'dep_name', 'cas_confirmes',\n",
    "               'cas_ehpad', 'cas_confirmes_ehpad', 'cas_possibles_ehpad', 'deces',\n",
    "               'deces_ehpad', 'reanimation', 'hospitalises',\n",
    "               'nouvelles_hospitalisations', 'nouvelles_reanimations', 'gueris',\n",
    "               'depistes', 'source_nom', 'source_url', 'source_archive', 'source_type']]\n",
    "\n",
    "        rs = []\n",
    "        for ind, row in df_clean.iterrows():   #[[\"deces\",\"gueris\", 'reanimation', 'hospitalises']]\n",
    "            if row[\"date\"].year == 2020:\n",
    "                effectif = pop.loc[pop['code_dpt'] == row['code_dep'], \"2020\"]\n",
    "            else: \n",
    "                effectif = pop.loc[pop['code_dpt'] == row['code_dep'], \"2021\"]\n",
    "            dc = row[\"deces\"] / effectif * 100000\n",
    "            gu = row[\"gueris\"] / effectif * 100000\n",
    "        #     rea = row[\"reanimation\"] / effectif * 100000\n",
    "            hos = row[\"hospitalises\"] / effectif * 100000\n",
    "        #     tot= dc.values[0] + gu.values[0] + hos.values[0]\n",
    "\n",
    "            rs.append([row[\"date\"], row[\"region_name\"], row[\"code_dep\"], row[\"dep_name\"], dc.values[0], gu.values[0], hos.values[0]])\n",
    "        col = ['date', 'region_name', 'dep_code', 'dep_name', 'deces', 'gueris', 'symptomatiques']            \n",
    "\n",
    "\n",
    "        final = pd.DataFrame(rs, columns = col)\\\n",
    "        .merge(quatre_taux[[\"extract_date\",'departement', 'tx_incid']], left_on=['date','dep_code'], right_on=[\"extract_date\",'departement'],suffixes=('_x', '_y'))\\\n",
    "        .drop(columns=[\"extract_date\",'departement'])#\\\n",
    "    #     .rename(columns= {\"tx_incid\" : \"susceptibles\"})\n",
    "\n",
    "        final[\"asymptomatiques\"] = final[\"tx_incid\"] - final['symptomatiques']\n",
    "\n",
    "    #     start_date = final[final['asymptomatiques']>0].sort_values('date').iloc[0,0]\n",
    "        final = final[final[\"date\"].duplicated()]\n",
    "        final.loc[(final['asymptomatiques']<0), \"asymptomatiques\"] = float('NaN')\n",
    "        t = final.groupby(\"dep_name\").size()\n",
    "        to_drop = t[t<50].index\n",
    "        final = final[~final[\"dep_name\"].isin(to_drop)]\n",
    "    #     final = final.loc[(final['date'] >= start_date)  , : ]\n",
    "\n",
    "        rs=[]\n",
    "        for name, data in final.groupby('dep_name'):\n",
    "            data = data.reset_index()\n",
    "    #         print(name)\n",
    "            data.sort_values('date', inplace=True)\n",
    "            if data['tx_incid'].isnull().loc[0]:\n",
    "                data.loc[0,'tx_incid'] = 0\n",
    "            if data['deces'].isnull().loc[0]:\n",
    "                data.loc[0,'deces'] = 0 \n",
    "            if data['gueris'].isnull().loc[0]:\n",
    "                data.loc[0,'gueris'] = 0 \n",
    "            if data['symptomatiques'].isnull().loc[0]:\n",
    "                data.loc[0,'symptomatiques'] = 0 \n",
    "            if data['asymptomatiques'].isnull().loc[0]:\n",
    "                data.loc[0,'asymptomatiques'] = 0 \n",
    "                data = data.set_index('date').interpolate(method='linear')\n",
    "                data = data[data.index.duplicated(keep=\"last\")].drop(columns='index')\n",
    "                rs.append(data)\n",
    "        final = pd.concat(rs)\n",
    "\n",
    "        return final\n",
    "\n",
    "    def load(file, path = \"../data/df_clean.csv\"):\n",
    "        if path = \"../data/df_clean.csv\":\n",
    "            os.rename(\"../data/df_clean.csv\", \"../data/_old_df_clean.csv\")\n",
    "        file.to_csv(path)\n",
    "\n",
    "    e = extract(local=False)\n",
    "    df = transform(e)\n",
    "    load(df)\n",
    "    \n",
    "schedule.every().day.at(\"02:00\").do(job)\n",
    "\n",
    "if __name__== '__main__':\n",
    "    while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danpdvn/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (17,18) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "def extract(local=True):\n",
    "\n",
    "    if local:\n",
    "        df = pd.read_csv(\"../data/chiffres-cles.csv\")\n",
    "        quatre_taux = pd.read_csv(\"../data/table-indicateurs-open-data-dep-serie.csv\")\n",
    "    else: \n",
    "        url = \"https://github.com/opencovid19-fr/data/raw/master/dist/chiffres-cles.csv\"\n",
    "        s = requests.get(url).content\n",
    "        df = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "        url= \"https://www.data.gouv.fr/fr/datasets/r/4acad602-d8b1-4516-bc71-7d5574d5f33e\"\n",
    "        s=requests.get(url).content\n",
    "        quatre_taux = pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "\n",
    "    dpt = pd.read_csv(\"../data/departements-region.csv\")\n",
    "    reg = pd.read_csv(\"../data/regions-france.csv\")\n",
    "    pop = pd.read_csv(\"../data/estim_pop.csv\")\n",
    "\n",
    "\n",
    "    df[\"date\"] = df[\"date\"].replace(\"_\", \"-\", regex=True)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    df[\"dep_code\"] = df[(df['maille_code'].str.startswith('DEP')) | (df['maille_code'].str.startswith('COM'))][\"maille_code\"]\\\n",
    "    .replace(\".*-\", \"\", regex=True)\n",
    "    df[\"reg_code\"] = df[df['maille_code'].str.startswith('REG')][\"maille_code\"]\\\n",
    "    .replace(\".*-\", \"\", regex=True).astype(\"int64\")\n",
    "\n",
    "    quatre_taux[\"extract_date\"] = pd.to_datetime(quatre_taux[\"extract_date\"])\n",
    "\n",
    "\n",
    "    geo = dpt.merge(reg, right_on= 'nom_region', left_on='region_name')\\\n",
    "    .rename(columns = {'num_dep':'code_dep'})\\\n",
    "    .drop(columns=[\"nom_region\"])[['code_region', 'region_name', 'code_dep', 'dep_name']]\n",
    "\n",
    "    pop[[\"2020\", \"2021\"]] = pop[[\"2020\", \"2021\"]].replace(\"\\s\", \"\", regex=True).astype(int)\n",
    "\n",
    "    return [df, quatre_taux, geo, pop]\n",
    "\n",
    "\n",
    "def transform(dataframe):\n",
    "\n",
    "    df, quatre_taux, geo, pop = dataframe[0], dataframe[1], dataframe[2], dataframe[3]\n",
    "\n",
    "    df_clean = df.merge(geo, right_on=\"code_dep\", left_on='dep_code')[['date', 'code_region', 'region_name', 'code_dep',\n",
    "           'dep_name', 'cas_confirmes',\n",
    "           'cas_ehpad', 'cas_confirmes_ehpad', 'cas_possibles_ehpad', 'deces',\n",
    "           'deces_ehpad', 'reanimation', 'hospitalises',\n",
    "           'nouvelles_hospitalisations', 'nouvelles_reanimations', 'gueris',\n",
    "           'depistes', 'source_nom', 'source_url', 'source_archive', 'source_type']]\n",
    "\n",
    "    rs = []\n",
    "    for ind, row in df_clean.iterrows():   #[[\"deces\",\"gueris\", 'reanimation', 'hospitalises']]\n",
    "        if row[\"date\"].year == 2020:\n",
    "            effectif = pop.loc[pop['code_dpt'] == row['code_dep'], \"2020\"]\n",
    "        else: \n",
    "            effectif = pop.loc[pop['code_dpt'] == row['code_dep'], \"2021\"]\n",
    "        dc = row[\"deces\"] / effectif * 100000\n",
    "        gu = row[\"gueris\"] / effectif * 100000\n",
    "    #     rea = row[\"reanimation\"] / effectif * 100000\n",
    "        hos = row[\"hospitalises\"] / effectif * 100000\n",
    "    #     tot= dc.values[0] + gu.values[0] + hos.values[0]\n",
    "\n",
    "        rs.append([row[\"date\"], row[\"region_name\"], row[\"code_dep\"], row[\"dep_name\"], dc.values[0], gu.values[0], hos.values[0]])\n",
    "    col = ['date', 'region_name', 'dep_code', 'dep_name', 'deces', 'gueris', 'symptomatiques']            \n",
    "\n",
    "\n",
    "    final = pd.DataFrame(rs, columns = col)\\\n",
    "    .merge(quatre_taux[[\"extract_date\",'departement', 'tx_incid']], left_on=['date','dep_code'], right_on=[\"extract_date\",'departement'],suffixes=('_x', '_y'))\\\n",
    "    .drop(columns=[\"extract_date\",'departement'])#\\\n",
    "#     .rename(columns= {\"tx_incid\" : \"susceptibles\"})\n",
    "\n",
    "    final[\"asymptomatiques\"] = final[\"tx_incid\"] - final['symptomatiques']\n",
    "\n",
    "#     start_date = final[final['asymptomatiques']>0].sort_values('date').iloc[0,0]\n",
    "    final = final[final[\"date\"].duplicated()]\n",
    "    final.loc[(final['asymptomatiques']<0), \"asymptomatiques\"] = float('NaN')\n",
    "    t = final.groupby(\"dep_name\").size()\n",
    "    to_drop = t[t<50].index\n",
    "    final = final[~final[\"dep_name\"].isin(to_drop)]\n",
    "#     final = final.loc[(final['date'] >= start_date)  , : ]\n",
    "\n",
    "    rs=[]\n",
    "    for name, data in final.groupby('dep_name'):\n",
    "        data = data.reset_index()\n",
    "#         print(name)\n",
    "        data.sort_values('date', inplace=True)\n",
    "        if data['tx_incid'].isnull().loc[0]:\n",
    "            data.loc[0,'tx_incid'] = 0\n",
    "        if data['deces'].isnull().loc[0]:\n",
    "            data.loc[0,'deces'] = 0 \n",
    "        if data['gueris'].isnull().loc[0]:\n",
    "            data.loc[0,'gueris'] = 0 \n",
    "        if data['symptomatiques'].isnull().loc[0]:\n",
    "            data.loc[0,'symptomatiques'] = 0 \n",
    "        if data['asymptomatiques'].isnull().loc[0]:\n",
    "            data.loc[0,'asymptomatiques'] = 0 \n",
    "            data = data.set_index('date').interpolate(method='linear')\n",
    "            data = data[data.index.duplicated(keep=\"last\")].drop(columns='index')\n",
    "            rs.append(data)\n",
    "    final = pd.concat(rs)\n",
    "\n",
    "    return final\n",
    "\n",
    "def load(file, path = \"../data/df_clean.csv\"):\n",
    "    if path == \"../data/df_clean.csv\":\n",
    "        os.rename(\"../data/df_clean.csv\", \"../data/_old_df_clean.csv\")\n",
    "    file.to_csv(path)\n",
    "\n",
    "e = extract(local=False)\n",
    "df = transform(e)\n",
    "#load(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "deriv_final = pd.concat([final.loc[:,:'dep_name'],final.loc[:, 'deces':].diff().fillna(0)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "SEAIR\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dep_name\n",
       "Ain                        0.000000\n",
       "Aisne                     -0.379385\n",
       "Allier                    -0.925217\n",
       "Alpes-Maritimes            0.000000\n",
       "Alpes-de-Haute-Provence   -0.242623\n",
       "                             ...   \n",
       "Var                        0.000000\n",
       "Vaucluse                  -0.011803\n",
       "Vendée                     0.090287\n",
       "Vienne                    -0.385000\n",
       "Vosges                    -0.230538\n",
       "Name: tx_incid, Length: 83, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deriv_final.groupby(['dep_name'])['tx_incid'].apply(lambda x: x.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = data.iloc[0, -4] / data.iloc[0, -2]\n",
    "beta = data.iloc[0, -2] * 1000\n",
    "gamma1 = data.iloc[0, -4] * data.iloc[0, -5]*(10**6)\n",
    "gamma2 = data.iloc[0, -1] * data.iloc[0, -5]*(10**6)\n",
    "delta = gamma1 / gamma2\n",
    "theta = data\n",
    "mu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEAIRD\n",
    "import numpy as np\n",
    "\n",
    "def params(data):\n",
    "    alpha = data.iloc[0, -4] / data.iloc[0, -2]\n",
    "    beta = data.iloc[0, -2] * 1000\n",
    "    gamma1 = data.iloc[0, -4] * data.iloc[0, -5]*(10**6)\n",
    "    gamma2 = data.iloc[0, -1] * data.iloc[0, -5]*(10**6)\n",
    "    delta = gamma1 / gamma2\n",
    "    theta = data.iloc[0, 4] * 1000\n",
    "    mu = 0\n",
    "\n",
    "def model(ini, time_step, params):\n",
    "    Y = np.zeros(6) #column vector for the state variables\n",
    "    S, E, I, A, R, D = ini\n",
    "    alpha = params[0]\n",
    "    beta = params[1]\n",
    "    delta = params[2]\n",
    "    gamma1 = params[3]\n",
    "    gamma2 =params[4]\n",
    "    theta = params[5]\n",
    "    mu = 0\n",
    "\n",
    "    Y[0] = - ( (beta*S*I + delta*A) / (S+E+I+A+R) ) - S + 100000 #S\n",
    "    Y[1] = ( (beta*S*I + delta*A) / (S+E+I+A+R) ) - (mu +1)*E #E\n",
    "    Y[2] = alpha*mu*E -(gamma1 + theta + 1)*I #I\n",
    "    Y[3] = (1-alpha)*mu*E -(gamma + 1)*A #A\n",
    "    Y[4] = gamma1*I + gamma2*A - R #R\n",
    "    Y[5] = theta*I #D\n",
    "\n",
    "    return Y\n",
    "\n",
    "def x0fcn(data):\n",
    "    data = data.iloc[0,:]\n",
    "    E0 = data[\"tx_incid\"]\n",
    "    A0 = data[\"asymptomatiques\"]\n",
    "    I0 = data[\"tx_incid\"] - data[\"asymptomatiques\"]\n",
    "    R0 = data[\"gueris\"]\n",
    "    D0 = data[\"deces\"]\n",
    "    S0 = 100000.0 - E0 - A0 - I0 - R0 - D0\n",
    "    X0 = [S0, E0, A0, I0, R0, D0]\n",
    "\n",
    "    return X0\n",
    "\n",
    "\n",
    "def yfcn(res, params):\n",
    "    return res[:,1]*params[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "from scipy.stats import norm\n",
    "\n",
    "from scipy.integrate import odeint as ode\n",
    "\n",
    "def NLL(params, data, times): #negative log likelihood\n",
    "    params = np.abs(params)\n",
    "    # \tdata = np.array(data)\n",
    "    res = ode(model, sir_ode.x0fcn(params,data), times, args=(params,))\n",
    "    y = yfcn(res, params)\n",
    "    nll = sum(y) - sum(data*np.log(y))\n",
    "    # note this is a slightly shortened version--there's an additive constant term missing but it \n",
    "    # makes calculation faster and won't alter the threshold. Alternatively, can do:\n",
    "    # nll = -sum(np.log(poisson.pmf(np.round(data),np.round(y)))) # the round is b/c Poisson is for (integer) count data\n",
    "    # this can also barf if data and y are too far apart because the dpois will be ~0, which makes the log angry\n",
    "\n",
    "    # ML using normally distributed measurement error (least squares)\n",
    "    # nll = -sum(np.log(norm.pdf(data,y,0.1*np.mean(data)))) # example WLS assuming sigma = 0.1*mean(data)\n",
    "    # nll = sum((y - data)**2)  # alternatively can do OLS but note this will mess with the thresholds \n",
    "    #                             for the profile! This version of OLS is off by a scaling factor from\n",
    "    #                             actual LL units.\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimum fischer matrix\n",
    "def minifisher (times, params, data, delta = 0.001):\n",
    "    #params = np.array(params)\n",
    "    listX = []\n",
    "    params_1 = np.array (params)\n",
    "    params_2 = np.array (params)\n",
    "    for i in range(len(params)):\n",
    "        params_1[i] = params[i] * (1+delta)\n",
    "        params_2[i]= params[i] * (1-delta)\n",
    "\n",
    "        res_1 = ode(model, x0fcn(params_1,data), times, args=(params_1,))\n",
    "        res_2 = ode(model, x0fcn(params_2,data), times, args=(params_2,))\n",
    "        subX = (yfcn(res_1, params_1) - yfcn(res_2, params_2)) / (2 * delta * params[i])\n",
    "        listX.append(subX.tolist())\n",
    "    X = np.matrix(listX)\n",
    "    FIM = np.dot(X, X.transpose())\n",
    "    return FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proflike (params, profindex, cost_func, times, data, perrange = 0.5, numpoints = 10):\n",
    "    profrangedown = np.linspace(params[profindex], params[profindex] * (1 - perrange), numpoints).tolist()\n",
    "    profrangeup = np.linspace(params[profindex], params[profindex] * (1 + perrange), numpoints).tolist()[1:] #skip the duplicated values\n",
    "    profrange = [profrangedown, profrangeup]\n",
    "    currvals = []\n",
    "    currparams = []\n",
    "    currflags = []\n",
    "\n",
    "    def profcost (fit_params, profparam, profindex, data, times, cost_func):\n",
    "        paramstest = fit_params.tolist()\n",
    "        paramstest.insert(profindex, profparam)\n",
    "        return cost_func (paramstest, data, times)\n",
    "\n",
    "    fit_params = params.tolist() #make a copy of params so we won't change the origianl list\n",
    "    fit_params.pop(profindex)\n",
    "    print('Starting profile...')\n",
    "    for i in range(len(profrange)):\n",
    "        for j in profrange[i]:\n",
    "            print(i, j)\n",
    "            optimizer = optimize.minimize(profcost, fit_params, args=(j, profindex, data, times, cost_func), method='Nelder-Mead')\n",
    "            fit_params = np.abs(optimizer.x).tolist() #save current fitted params as starting values for next round\n",
    "            #print optimizer.fun\n",
    "            currvals.append(optimizer.fun)\n",
    "            currflags.append(optimizer.success)\n",
    "            currparams.append(np.abs(optimizer.x).tolist())\n",
    "\n",
    "    #structure the return output\n",
    "    profrangedown.reverse()\n",
    "    out_profparam = profrangedown+profrangeup\n",
    "    temp_ind = range(len(profrangedown))\n",
    "    temp_ind.reverse()\n",
    "    out_params = [currparams[i] for i in temp_ind]+currparams[len(profrangedown):]\n",
    "    out_fvals = [currvals[i] for i in temp_ind]+currvals[len(profrangedown):]\n",
    "    out_flags = [currflags[i] for i in temp_ind]+currflags[len(profrangedown):]\n",
    "    output = {'profparam': out_profparam, 'fitparam': np.array(out_params), 'fcnvals': out_fvals, 'convergence': out_flags}\n",
    "    return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
